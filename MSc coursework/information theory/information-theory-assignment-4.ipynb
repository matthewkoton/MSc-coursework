{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "49eaf30b",
      "metadata": {
        "id": "49eaf30b"
      },
      "source": [
        "##### Introduction to Information Theory (Fall 2023/4)\n",
        "\n",
        "# Home Assignment 4\n",
        "\n",
        "#### Topics:\n",
        "- Lossless compression\n",
        "\n",
        "#### Due: 20/2/2024 before the class\n",
        "\n",
        "#### Instructions:\n",
        "- Write your names and date in the cell below.\n",
        "- Submit a copy of this notebook with code filled in the relevant places as the solution to coding exercises.\n",
        "- You are welcomed to ask for hints, clarifications, or report issues on **Piazza**.\n",
        "- For theoretic exercises, you can either write your solution in the notebook using $\\LaTeX$ (recommended) or submit additional notes.\n",
        "- Logarithm is in base $2$ unless stated otherwise.\n",
        "- For chain of equalities or inequalities, make sure to explain every non-trivial transition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1137232",
      "metadata": {
        "id": "c1137232"
      },
      "source": [
        "**Date**:\n",
        "\n",
        "**Name1**: Rebecca Menasci\n",
        "\n",
        "**Student ID1**: 806773\n",
        "\n",
        "**Name2**: Matthew Koton\n",
        "\n",
        "**Student ID2**: 806614"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77440392",
      "metadata": {
        "id": "77440392"
      },
      "source": [
        "### 1. Operational Capacity of a Noiseless Channel\n",
        "1. Prove that for any channel $P_{X^n|Y^n}$ we have $C_{op} \\leq  \\log|\\mathcal X|$ without using the channel coding theorem (That is, directly from the definition.). Use the following steps:\n",
        " - Suppose that there exists a $(2^{nR},n)$-coding scheme with $R >\\log|\\mathcal X|$. Find the number of messages that cannot be uniquely mapped to a channel input.\n",
        " - Find the proportion of this number of messages out of the entire message space.\n",
        " - Argue that the average error in decoding is at least half this proportion.\n",
        " - Complete the proof.\n",
        "\n",
        "2. Consider a channel with identical input and output alphabets $\\mathcal X$. Assume that for some $m \\in \\mathbb N$ the channel is noiseless, i.e.\n",
        "$$\n",
        "P_{Y^m|X^m}(y^m|x^m) = \\begin{cases} 1 & x^m = y^m \\\\\n",
        "0 & x^m \\neq y^m.\n",
        "\\end{cases}\n",
        "$$\n",
        "Show that the operational capacity of this channel is $C_{op} := \\log |\\mathcal X|$ by proving an achievability and converse claims (you are not allowed to use the channel coding theorem).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer question 1.1\n",
        "To demonstrate that for any channel characterized by $P_{X^n|Y^n}$, the operational capacity $C_{op}$ does not exceed $log|\\mathcal{X}|$, we proceed through the following logical steps:\n",
        "\n",
        "${Initial Assumption}$\n",
        "\n",
        "Consider a coding scheme described by $(2^{nR}, n)$ where $R$ exceeds $log|\\mathcal{X}|$. This implies an attempt to encode $2^{nR}$ distinct messages over $n$ channel uses. Since $R > \\log|\\mathcal{X}|$, we have $2^{nR} > |\\mathcal{X}|^n$.\n",
        "\n",
        "${Uniqueness of Message Mapping}$\n",
        "\n",
        "With $|\\mathcal{X}|^n$ being the total number of unique input sequences over $n$ channel uses and $2^{nR}$ messages to transmit, the pigeonhole principle ensures that certain messages will be assigned the same input sequence. The count of such non-uniquely mapped messages is at least $2^{nR} - |\\mathcal{X}|^n$.\n",
        "\n",
        "${Proportion Calculation}$\n",
        "\n",
        "The fraction of messages that are not uniquely mapped within the total message set is no less than $\\frac{2^{nR} - |\\mathcal{X}|^n}{2^{nR}}$, simplifying to $1 - \\frac{|\\mathcal{X}|^n}{2^{nR}}$.\n",
        "\n",
        "${Decoding Error Estimation}$\n",
        "\n",
        "Due to the mapping overlap, a portion of the messages will inevitably be decoded incorrectly. A conservative estimate of the decoding error rate would be at least half the proportion of these non-uniquely mapped messages, resulting in a minimum error rate of $\\frac{1}{2}\\left(1 - \\frac{|\\mathcal{X}|^n}{2^{nR}}\\right)$.\n",
        "\n",
        "${Conclusion of Proof}$\n",
        "\n",
        "A coding scheme's reliability necessitates that the decoding error rate approaches zero as $n$ grows indefinitely. However, for $R > \\log|\\mathcal{X}|$, the proportion of overlapping messages—and consequently the minimum error rate—does not converge to zero. This contradiction invalidates the existence of a reliable $(2^{nR}, n)$-coding scheme for $R > \\log|\\mathcal{X}|$, thereby establishing that $C_{op} \\leq \\log|\\mathcal{X}|$.\n",
        "\n",
        "\n",
        "\n",
        "### Answer question 1.2\n",
        "For a channel where the input and output alphabets $\\mathcal{X}$ are identical, and the channel behaves noiselessly for some $m \\in \\mathbb{N}$, such that $P_{Y^m|X^m}(y^m|x^m) = 1$ if $x^m = y^m$ and 0 otherwise, we aim to show $C_{op} = \\log |\\mathcal{X}|$.\n",
        "\n",
        "${Achievability Argument}$\n",
        "\n",
        "In a noiseless channel, every input sequence $x^m$ is perfectly transmitted to the output, resulting in $y^m = x^m$. For such a channel, we can design a coding scheme where each message is directly mapped to a unique sequence in $\\mathcal{X}^m$. Since there are $|\\mathcal{X}|^m$ unique sequences possible, and each sequence can represent a unique message, we can transmit $\\log(|\\mathcal{X}|^m) = m\\log|\\mathcal{X}|$ bits of information in $m$ uses of the channel.\n",
        "As $n$ approaches infinity, the rate $R = \\frac{m\\log|\\mathcal{X}|}{m} = \\log|\\mathcal{X}|$ can be achieved with zero error because the channel is noiseless. This demonstrates that $C_{op} \\geq \\log |\\mathcal{X}|$.\n",
        "\n",
        "\n",
        "${Converse Argument}$\n",
        "\n",
        "As established earlier, for any channel, $C_{op}$ is bounded above by $\\log|\\mathcal{X}|$. This upper limit is universally applicable, regardless of noise conditions. Hence, even in a noiseless setting, this capacity cannot be surpassed, indicating $C_{op} \\leq \\log|\\mathcal{X}|$.\n",
        "\n",
        "Combining the achievability and converse arguments confirms that for a noiseless channel with matching input and output alphabets, $C_{op}$ is precisely $\\log |\\mathcal{X}|$."
      ],
      "metadata": {
        "id": "6R0-h3gP9LT4"
      },
      "id": "6R0-h3gP9LT4"
    },
    {
      "cell_type": "markdown",
      "id": "d8a5526f",
      "metadata": {
        "id": "d8a5526f"
      },
      "source": [
        "### 2. Channel Capacity\n",
        "(Based on Exc. 7.6 in Thomas \\& Cover) Consider a 26-key typewriter.\n",
        "1. If pushing a key results in printing the associated letter, what is the capacity $C$ in bits?\n",
        "2. Now suppose that pushing a key results in printing the associated letter or the next letter in the alphabet with equal probability. That is, $A \\to B$, $B\\to C$,...,$Z \\to A$. What is the capacity?\n",
        "3. What is the highest rate code with block length one $(n=1)$ that you can find that achieves *zero* probability of error for the channel in part (2)?\n",
        "\n",
        "*Hint for (2)*:\n",
        "Show first that for a channel with transition matrix in which the rows are permutations of each other and the columns are permutations of each other, the capacity is\n",
        "$$\n",
        "C = \\log |\\mathcal Y| - H( \\text{row of transition matrix}).\n",
        "$$\n",
        "(such a channel is called *symmetric*)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answers\n",
        "1)\n",
        "\n",
        "$max_{p(x)}I(X:Y) = max_{p(x)}(H(y)-H(Y|X)) = max_{p(x)}H(y)$\n",
        "we get the max when it is uniform distribution therefor we get $log(26)$\n",
        "\n",
        "2)\n",
        "\n",
        "$max_{p(x)}I(X:Y) = max_{p(x)}(H(y)-H(Y|X))$\n",
        "\n",
        "$H(Y|X) = -(1/2)log(1/2)-(1/2)log(1/2) = -log(1/2) = log2$\n",
        "\n",
        "therefor we get $max_{p(x)}(H(y)-log2)$\n",
        "\n",
        "the max is attained when it is uniform therefor we get $log26-log2 =log(26/2) =log(13)$\n",
        "\n",
        "3)\n",
        "\n",
        "we take a subset of letters such that none of them can be flipped to eachother.\n",
        "For example we take {a,c,e,g,...} or {b,d,f,h...}\n",
        "\n",
        "our code encodes x-->x for all x in our subset.  we can decode (a,b) to a and (c,d) to c and so on\n",
        "\n",
        "we therefor get a rate of $\\frac{info}{n} = \\frac{info}{1} = \\frac{-log(1/13)}{1}$ = log(13) This is clearly the best rate as increasing the rate will nececarily result in having some probability > 0 of error."
      ],
      "metadata": {
        "id": "ceKZhm8aNdww"
      },
      "id": "ceKZhm8aNdww"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ds9SFR8R8cms"
      },
      "source": [
        "### Channel Capacity and Random Codes\n",
        "(based on Exc. 7.8 and 7.9 in Thomas \\& Cover)\n",
        "The $Z$-channel has a binary input and output alphabets and transition probabilities $P_{Y|X}$ given by the matrix\n",
        "$$\n",
        "P_{Y|X} = \\begin{bmatrix} 1 & 0 \\\\\n",
        "1/2 & 1/2\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "Namely, $\\Pr[Y=0|X=0]=1$ while $\\Pr[Y=0|X=1]=1/2$ ($0$ goes noiselessly, while a $1$ may turn into a zero with probability $1/2$).\n",
        "1. Find the capacity of this channel and the maximizing input probability distribution. It may help to know that\n",
        "$$\n",
        "\\frac{d}{dp} h_2(p) = \\log_2((1-p)/p).\n",
        "$$\n",
        "2. Assume that we draw a random $(2^{nR},n)$ code (as in the proof of the channel coding theorem) for this channel in which each codeword is a sequence of *fair* coin tosses (this may not achieve capacity). Find the maximum rate $R$ such that the probability of error $P_{\\mathrm{err}}^{(n)}$ averaged over the randomly generated codes, tends to zero as $n$ tends to infinity.\n",
        "\n"
      ],
      "id": "Ds9SFR8R8cms"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answers\n",
        "1)\n",
        "\n",
        "let p = P(x=1)\n",
        "\n",
        "I(X:Y)\n",
        "\n",
        "$max_{p(x)}I(X:Y) = max_{p(x)}H(Y)-H(Y|X)$\n",
        "\n",
        "H(Y|X) = (p(-(1/2)log(1/2) - (1/2)log(1/2)) + (1-p)(0)) (no entropy when x is 0)\n",
        "\n",
        "→ H(Y|X) = p\n",
        "\n",
        "p(y=0) = (1-p)1 + p/2 = 1-p/2\n",
        "\n",
        "p(y=1) = (1-p)0 + p/2 = p/2\n",
        "\n",
        "therefor: $max_{p(x)}H(Y)-H(Y|X) = h_2(p/2)-p$\n",
        "\n",
        "→ $\\frac{d}{dp}(h_2(p/2)-p) = log(\\frac{1-p/2}{p/2})/2 - 1 = log(\\frac{2-p}{p})/2  - 1$\n",
        "\n",
        "we then get max at p = 2/5\n",
        "\n",
        "therefor the chanel capacity is $h_2(2/10)-2/5 = 0.32$\n",
        "\n",
        "\n",
        "\n",
        "2)\n",
        "\n",
        "Assume each codeword is a sequence of fair coin tosses of length n\n",
        "\n",
        "therefor each letter in our word we recieve will be 0 with probability 1/2*1/2 + 1*1/2=3/4 and 1 with probability 1/2*1/2 = 1/4\n",
        "\n",
        "By channel coding theorem we can find out max rate by calculating H(Y)-H(Y|X)\n",
        "\n",
        "H(Y) = -(3/4log(3/4)+1/4log(1/4))\n",
        "\n",
        "From above we get:  H(Y|X) = p = 0.5\n",
        "\n",
        "therefor we get that the max rate = H(Y)-H(Y|X) =\n",
        "-(3/4log(3/4)+1/4log(1/4)) - 0.5 = 0.31127\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GhID8j20OJjZ"
      },
      "id": "GhID8j20OJjZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}