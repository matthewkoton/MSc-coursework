{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWUerkMlU8x5"
   },
   "source": [
    "Matthew Koton\n",
    "\n",
    "\n",
    "Nofar Yungman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geWm22uFz2_G"
   },
   "source": [
    "# **Initial Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2N5Vtoqz0N4"
   },
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark\n",
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZAq2Mdfz-hu"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSxIHyjb0DEb"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uy9dFLdJ0GkB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQfH49qi0MSb"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CrprxaWN0NwK"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('treatmeant').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mn8ej7Wt0Gxl"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Vp02U93q5Ef"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3i9yVmM0bql"
   },
   "source": [
    "# **Read Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GBWks_X80irM"
   },
   "outputs": [],
   "source": [
    "df_train = spark.read.csv(\"/content/drive/MyDrive/Big data/hw3/HW3-sparkML/bdp_train.csv\", inferSchema=True, header=True)\n",
    "df_train_size = df_train.count()\n",
    "num_columns_original = len(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXZgdRSXjsvm"
   },
   "source": [
    "# **Null columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAWKuIVV2aiU"
   },
   "outputs": [],
   "source": [
    "null_counts = df_train.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in df_train.columns]).collect()[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQ1M0yNR2sih"
   },
   "outputs": [],
   "source": [
    "null_threshold_percent = 0.1\n",
    "##null_threshold_percent = 1\n",
    "\n",
    "to_drop = [k for k, v in null_counts.items() if (v / df_train_size) >= null_threshold_percent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Q14FiWa3KRI"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop(*to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kHr7x1qM20Ix",
    "outputId": "61c897f9-33eb-45f8-c3eb-aac27812404d"
   },
   "outputs": [],
   "source": [
    "# print threashold percentage\n",
    "print(\"threashold percentage:\", null_threshold_percent)\n",
    "\n",
    "# Print original number of columns\n",
    "print(\"Number of columns in the original DataFrame:\", num_columns_original)\n",
    "\n",
    "# Print the number of columns dropped\n",
    "num_columns_after_drop = len(df_train.columns)\n",
    "print(\"Number of columns dropped:\", num_columns_original - num_columns_after_drop)\n",
    "\n",
    "# Print the number of columns in the new DataFrame\n",
    "print(\"Number of columns in the new DataFrame:\", num_columns_after_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8NuK3IT5Dc_"
   },
   "outputs": [],
   "source": [
    "# print schema of our df after dropping null columns\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Brg_qfKpUqH"
   },
   "source": [
    "# **Null rows**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1w55Z-j6pW-C",
    "outputId": "f3b9e20f-8c14-4442-9662-c9dbc5e1deb3"
   },
   "outputs": [],
   "source": [
    "#threshold = int(len(df_train.columns)*0.1)\n",
    "#print(threshold)\n",
    "#print(df_train.count())\n",
    "#cols_minus_index = [col for col in df_train.columns if col != \"Index\"]\n",
    "#df_train = df_train.dropna(how='any', thresh=threshold, subset=cols_minus_index)\n",
    "print(df_train.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CX7Rups-50Tt"
   },
   "source": [
    "# **Get catagorical and numeric columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9tRzxKn53rI"
   },
   "outputs": [],
   "source": [
    "column_types = df_train.dtypes\n",
    "\n",
    "str_column_names = [f\"A{i}\" for i in range(1, 84)]\n",
    "non_str_column_names = [f\"A{i}\" for i in range(84, 453)]\n",
    "\n",
    "numerical_columns = []\n",
    "categorical_columns = []\n",
    "\n",
    "for col_name, col_type in column_types:\n",
    "  if (col_name in str_column_names):\n",
    "    categorical_columns.append(col_name)\n",
    "    if(col_type != 'string'):\n",
    "      df_train = df_train.withColumn(col_name, df_train[col_name].cast(\"string\"))\n",
    "\n",
    "  if (col_name in non_str_column_names):\n",
    "    numerical_columns.append(col_name)\n",
    "    if (col_type != 'int' or col_type != 'double'):\n",
    "      df_train = df_train.withColumn(col_name, df_train[col_name].cast(\"int\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Jnj_7hU-vu6"
   },
   "source": [
    "# **Create Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfs5_ovdA4fK"
   },
   "source": [
    "<u>changing features</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qblx5Qh-9SE"
   },
   "outputs": [],
   "source": [
    "categorical_columns_indexed = [c + \"_index\" for c in categorical_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q5J9l5su-zQt"
   },
   "outputs": [],
   "source": [
    "indexers = []\n",
    "\n",
    "for column in categorical_columns:\n",
    "    indexer = StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "    indexer.setHandleInvalid(\"keep\")\n",
    "    indexers.append(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wKHk3QqMUMmX"
   },
   "outputs": [],
   "source": [
    "\n",
    "imputer_numerical = Imputer(\n",
    "    inputCols= numerical_columns,\n",
    "    outputCols=[f\"{x}_imputed\" for x in numerical_columns],\n",
    "    strategy=\"mean\"\n",
    ")\n",
    "\n",
    "imputer_catagorical = Imputer(\n",
    "    inputCols= categorical_columns_indexed,\n",
    "    outputCols=[f\"{x}_imputed\" for x in categorical_columns_indexed],\n",
    "    strategy=\"mode\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMyL9gUFiXsJ"
   },
   "outputs": [],
   "source": [
    "features_combined = [f\"{x}_imputed\" for x in numerical_columns] + [f\"{x}_imputed\" for x in categorical_columns_indexed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9M4s4BHcNPpJ"
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=features_combined, outputCol=\"features_asm\", handleInvalid='keep' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuHe7RtwviDz",
    "outputId": "b9f32271-abf2-42ce-debd-e81dd7f7431d"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA, StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_asm\", outputCol=\"scaledFeatures\", withStd=False, withMean=True)\n",
    "\n",
    "\n",
    "pca = PCA(k=70, inputCol=\"scaledFeatures\")\n",
    "pca.setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfHPYoMAA-mG"
   },
   "source": [
    "<u>Create ML model</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b24FRqR2BEe7"
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(labelCol='CLASSIndex', featuresCol='features', numTrees=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WggbMGXsYWS"
   },
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBClassifier\n",
    "spark_reg_estimator = SparkXGBClassifier(\n",
    "  features_col=\"features\",\n",
    "  label_col=\"CLASSIndex\",\n",
    "  num_workers=2,\n",
    "  learning_rate = 0.01,\n",
    "  max_depth = 7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y6nLxMSmO7wB",
    "outputId": "055d82b7-4baf-4fae-84e7-fc45cd9589db"
   },
   "outputs": [],
   "source": [
    "spark_reg_estimator.getParam(\"learning_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m8jPJUTgp9mu"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "#paramGrid = ParamGridBuilder()\\\n",
    "#  .addGrid(spark_reg_estimator.max_depth, [2, 5])\\\n",
    "#  .addGrid(spark_reg_estimator.n_estimators, [10, 100])\\\n",
    "#  .build()\n",
    "\n",
    "\n",
    "\n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "#    .addGrid(spark_reg_estimator.learning_rate, [0.05, 0.1]) \\\n",
    "#    .addGrid(spark_reg_estimator.max_depth, [5, 75,150]) \\\n",
    "#    .build()\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(spark_reg_estimator.learning_rate, [0.01]) \\\n",
    "    .addGrid(spark_reg_estimator.max_depth, [5,7]) \\\n",
    "    .build()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "binary_eval = BinaryClassificationEvaluator(labelCol='CLASSIndex')\n",
    "\n",
    "# Declare the CrossValidator, which performs the model tuning.\n",
    "cv = CrossValidator(estimator=spark_reg_estimator, evaluator=binary_eval, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKAWee5vIbOq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3jFsFvhBCqp"
   },
   "source": [
    "<u>Define pipeline</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddQrYHyk_q_v"
   },
   "outputs": [],
   "source": [
    "#pipeline = Pipeline(stages=indexers + [imputer_numerical, imputer_catagorical, assembler, scaler, pca, spark_reg_estimator])\n",
    "pipeline = Pipeline(stages=indexers + [imputer_numerical, imputer_catagorical, assembler, scaler, pca, cv])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOqkXt19DEd7"
   },
   "source": [
    "# **Fit pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypojsRCBDMqM"
   },
   "outputs": [],
   "source": [
    "# create class indexer\n",
    "indexer = StringIndexer(inputCol='CLASS', outputCol='CLASSIndex')\n",
    "df_train = indexer.fit(df_train).transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vnuIZzm6Goow"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "print(df_train.count())\n",
    "\n",
    "# remove null rows\n",
    "df_train = df_train.na.drop()\n",
    "print(df_train.count())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QyMSfmp3JNTb"
   },
   "outputs": [],
   "source": [
    "# split data\n",
    "train_data, val_data = df_train.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L8kPoo2BD2SM",
    "outputId": "03dc89a9-9e6d-4a86-a6e9-757fe9297518"
   },
   "outputs": [],
   "source": [
    "pipeline = pipeline.fit(train_data)\n",
    "#pipeline = pipeline.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q15xLvVbVau3",
    "outputId": "35b56f09-7ed7-4368-d593-c475eade9b33"
   },
   "outputs": [],
   "source": [
    "cv_model = pipeline.stages[-1]\n",
    "best_model = cv_model.bestModel\n",
    "\n",
    "# Assuming 'learningRate' and 'n_estimators' are parameters of the model\n",
    "best_learning_rate = best_model.getOrDefault('learning_rate')\n",
    "best_n_estimators = best_model.getOrDefault('max_depth')\n",
    "\n",
    "print(best_learning_rate, best_n_estimators)\n",
    "# 0.1, 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBR4hl-lI6Sn"
   },
   "source": [
    "# **transform pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pNvUFMeqI-zl"
   },
   "outputs": [],
   "source": [
    "predictions_df = pipeline.transform(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pQMtyG1kJCko",
    "outputId": "76d576bd-eb72-444f-a79e-44edbba33dd9"
   },
   "outputs": [],
   "source": [
    "binary_eval = BinaryClassificationEvaluator(labelCol='CLASSIndex')\n",
    "print('RFC prediction AUC:', binary_eval.evaluate(predictions_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtsLjLv7J8jx"
   },
   "source": [
    "# **Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8yBzd1KhKAUd"
   },
   "outputs": [],
   "source": [
    "df_test = spark.read.csv(\"/content/drive/MyDrive/Big data/hw3/HW3-sparkML/bdp_test.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5D6ZvCVjJxV"
   },
   "outputs": [],
   "source": [
    "column_types_test = df_test.dtypes\n",
    "\n",
    "for col_name, col_type in column_types_test:\n",
    "  if (col_name in str_column_names):\n",
    "    if(col_type != 'string'):\n",
    "      df_test = df_test.withColumn(col_name, df_test[col_name].cast(\"string\"))\n",
    "\n",
    "  if (col_name in non_str_column_names):\n",
    "    if (col_type != 'int' or col_type != 'double'):\n",
    "      df_test = df_test.withColumn(col_name, df_test[col_name].cast(\"int\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CdNm3ywTldOS",
    "outputId": "85651bc2-4c2e-4fa7-d884-5c6961405d87"
   },
   "outputs": [],
   "source": [
    "print(df_test.count())\n",
    "print(len(df_test.columns))\n",
    "\n",
    "print(df_train.count())\n",
    "print(len(df_train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qw9kyKfuKD5D"
   },
   "outputs": [],
   "source": [
    "test_predictions_df = pipeline.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvD3mFr_KWw5"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array\n",
    "test_predictions_df = test_predictions_df.withColumn('ProbToYes', vector_to_array(f.col('probability')).getItem(1)).select('index', 'ProbToYes')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RDxlxsduLG8C",
    "outputId": "2ddbb820-d8f4-4e04-c615-0a8937272897"
   },
   "outputs": [],
   "source": [
    "print(test_predictions_df.printSchema())\n",
    "print(test_predictions_df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gw2C1uhKNZ4"
   },
   "outputs": [],
   "source": [
    "test_predictions_df.coalesce(1).write.mode('overwrite').csv('/content/drive/MyDrive/Big data/hw3/HW3-sparkML/pred-no-cv.csv', header = 'true')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
